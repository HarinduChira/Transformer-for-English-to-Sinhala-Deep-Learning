{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"D9rHBNX-yxKM"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Model\n","from keras.layers import Input, Embedding, Dense, Masking\n","from keras.layers import Attention, LayerNormalization, Dropout\n","from keras.optimizers import Adam\n","import os\n","import tensorflow as tf"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"zVVxY3ujzFZs","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a77f8332-9e4f-453d-e25e-403ef30ac654","executionInfo":{"status":"ok","timestamp":1721659513864,"user_tz":-330,"elapsed":24540,"user":{"displayName":"Harindu Basnayake","userId":"07786780909468288837"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["os.chdir(\"/content/drive/My Drive/Colab Notebooks/Deep/English_Sinhala_Transfomer\")\n","os.getcwd()"],"metadata":{"id":"ETwXiUN6zHsq","colab":{"base_uri":"https://localhost:8080/","height":158},"outputId":"0a1304a6-393e-43fa-d56c-71e94637eea3","executionInfo":{"status":"error","timestamp":1721659535528,"user_tz":-330,"elapsed":1411,"user":{"displayName":"Harindu Basnayake","userId":"07786780909468288837"}}},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/My Drive/Colab Notebooks/Deep/English_Sinhala_Transfomer'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-6450751b3de0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/Colab Notebooks/Deep/English_Sinhala_Transfomer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Colab Notebooks/Deep/English_Sinhala_Transfomer'"]}]},{"cell_type":"code","source":["df=pd.read_csv('/content/drive/My Drive/Colab Notebooks/Deep/English_Sinhala_Transfomer/sinhala.csv')"],"metadata":{"id":"7ghbNVa-8E1E","colab":{"base_uri":"https://localhost:8080/","height":287},"executionInfo":{"status":"error","timestamp":1721659595040,"user_tz":-330,"elapsed":673,"user":{"displayName":"Harindu Basnayake","userId":"07786780909468288837"}},"outputId":"ab20bece-1245-4c02-d0d7-0bd08f093f2d"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/My Drive/Colab Notebooks/Deep/English_Sinhala_Transfomer/sinhala.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-04506dd62277>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/Colab Notebooks/Deep/English_Sinhala_Transfomer/sinhala.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Colab Notebooks/Deep/English_Sinhala_Transfomer/sinhala.csv'"]}]},{"cell_type":"code","source":["spa_sentences = df[\"sinhala\"] + \"\\t\" + df[\"english\"]\n","\n","# Save the concatenated sentences to a text file\n","with open(\"spa.txt\", \"w\", encoding=\"utf-8\") as file:\n","    for sentence in spa_sentences:\n","        file.write(str(sentence) + \"\\n\")"],"metadata":{"id":"w582iaWz8hlc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_file =\"spa.txt\""],"metadata":{"id":"K1JULi9f-5zw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"mNamCma-F6SP"}},{"cell_type":"code","source":["text_file =\"spa.txt\"\n","with open(text_file) as f:\n","  lines=f.read().split(\"\\n\")[:-1]"],"metadata":{"id":"8jN-n1BQAQK5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["i=0\n","for line in lines:\n","  print(line)\n","  i=i+1\n","  if(i==20):\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"af1PEoZNAXz2","outputId":"15d89aa5-f2f0-4e61-ce9d-af0192faa2ef","executionInfo":{"status":"ok","timestamp":1713695040118,"user_tz":-330,"elapsed":5,"user":{"displayName":"Harindu Chiranjaya","userId":"07786780909468288837"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ඔයාට තැපෑලෙන් පැකේජ් එකක් හම්බවේවි\tyou will receive a package in the mail\n","ලියාපදිංචිය සම්පූර්ණ කලාට පස්සෙ ඔයාට තහවුරු කිරීමේ කේතයක්  හම්බවේවි\tyou will receive a confirmation code after completing the registration\n","ඊලඟ මිලදී ගැනීම කරන කොට ඔයාට වට්ටමක් හම්බවේවි\tyou will receive a discount on your next purchase\n","ඔයාට අපේ පාරිභෝගික සේවා කණ්ඩායමෙන් දුරකථන ඇමතුමක් හම්බවේවි\tyou will receive a phone call from our customer service team\n","ඔයාගේ ඇණවුම සූදානම් උනාට පස්සෙ ඔයාට දැනුම් දීමක් හම්බවේවි\tyou will receive a notification when your order is ready for pickup\n","ඔයාට පැය 24ක් ඇතුලත ඔයාගෙ විමසීමට පිළිතුරක්  හම්බවේවි\tyou will receive a response to your inquiry within 24 hours\n","ඔයාට ඔයාගේ පක්ෂපාතිත්වය වෙනුවෙන් තෑග්ගක් හම්බවේවි\tyou will receive a gift for your loyalty\n","ඔයාට උත්සවයට ආරාධනාවක් හම්බවේවි\tyou will receive an invitation to the event\n","ඔයාට රිටන් කරන භාණ්ඩය වෙනුවෙන් ආපසු මුදල් ගෙවීමක් හම්බවේවි\tyou will receive a refund for the returned item\n","ඔයාට විස්තර එක්ක  ඊමේල් එකක් හම්බවේවි\tyou'll receive an email with the details\n","ඔයාට තැපෑලෙන් පැකේජ් එකක් හම්බවේවි\tyou'll receive a package in the mail\n","ඔයාට රිටන් කරන භාණ්ඩය වෙනුවෙන් ආපසු මුදල් ගෙවීමක් හම්බවේවි\tyou'll receive a refund for the returned item\n","ඔයාට ඔයාගේ පක්ෂපාතිත්වය වෙනුවෙන් තෑග්ගක් හම්බවේවි\tyou'll receive a gift for your loyalty\n","ඔයාගෙ අම්මට කාර් එකක් හම්බවේවි\tyour mother will receive a car\n","ඔයාගෙ සහෝදරයට කාර් එකක් හම්බවේවි\tyour brother will receive a car\n","ඒක මාර ලස්සනයි\tit's pretty cool\n","අපි ඔයාට ආරාධනා කරනවා ඒක කරන්න කියලා\twe're inviting you to do that\n","යන්ත්‍ර පරිවර්තනය දියුණු කරන්න අපිත් එක්ක එකතු වෙන්න\tjoin us in improving machine translation\n","ඔයාගෙ පරිවර්තන NMT මොඩලය පුහුණු කරන්න පාවිච්චි කරාවි\tyour translations will be used to train an nmt model\n","අපි ඔයාට ආරාධනා කරනවා  ZoomNMT පුහුණු කරන්න පරිවර්තන සාම්පල ඉදිරිපත් කරන්න කියලා\twe're inviting you to submit translation samples for the zoomnmt training\n"]}]},{"cell_type":"code","source":["for x in range(len(lines)-10,len(lines)):\n","  print(lines[x])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"okVVuwCiAcTd","outputId":"670f33a8-69b1-40dd-b6e4-c338f4401eb8","executionInfo":{"status":"ok","timestamp":1713695043790,"user_tz":-330,"elapsed":6,"user":{"displayName":"Harindu Chiranjaya","userId":"07786780909468288837"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["එක් කලෙක කුඩා ගමක ලිලී නම් තරුණියක් ජීවත් වූ අතර ඇයට ලස්සනම මල් වගා කිරීම සඳහා විශේෂ තෑග්ගක් තිබුණි\tOnce upon a time in a small village there lived a young girl named Lily who had a special gift for growing the most beautiful flowers\n","දුර රටක ශ්‍රීමත් ආතර් නම් නිර්භීත නයිට්වරයා නපුරු මායාකාරයෙකුගෙන් පැහැරගෙන ගිය කුමරියක් බේරා ගැනීමට භයානක ගමනක් ආරම්භ කළේය\tIn a distant land a brave knight named Sir Arthur embarked on a perilous journey to rescue a kidnapped princess from an evil sorcerer\n","කුතුහලය දනවන ගවේෂකයෙකු වන එමිලි ඇමසන් වනාන්තරයේ ගැඹුරින් ගිලිහී ගිය පැරණි නගරයක සැඟවුණු වස්තු සොයා ගැනීමට ත්‍රාසජනක ගමනක් ආරම්භ කළාය\tEmily a curious explorer set out on an adventure to uncover the hidden treasures of an ancient lost city deep in the Amazon rainforest\n","සාමකාමී නගරයක් වන විලෝබෲක්හි ඔලිවර් නම් දඟකාර බළලාට විනෝදජනක සහ අනපේක්ෂිත දුෂ්කරතාවන්ට පත්වීමේ දක්ෂතාවයක් තිබුණි\tIn the peaceful town of Willowbrook a mischievous cat named Oliver had a talent for getting into amusing and unexpected predicaments\n","දක්ෂ පියානෝ වාදකයෙකු වන සාරා Carnegie Hall හි මහා වේදිකාවේ රඟ දැක්වීමට සිහින මැවූ අතර ඇගේ සංගීත ශිල්පය පුහුණු කිරීමට පැය ගණන් ගත කළාය\tSarah a talented pianist dreamt of performing on the grand stage of Carnegie Hall and spent countless hours practicing her musical craft\n","වශීකෘත වනාන්තරයේ ගැඹුරින් නුවණැති මහලු බකමූණෙකු විසින් මෙහෙයවන ලද වනාන්තර ජීවීන් කණ්ඩායමක් තම නිවස විනාශයෙන් බේරා ගැනීමේ ගවේෂණයක යෙදී සිටියහ\tDeep in the enchanted forest a group of woodland creatures led by a wise old owl embarked on a quest to save their home from destruction\n","අව්ව සහිත ගිම්හාන දිනයක මිතුරන් පිරිසක් වැලි මාලිගා ගොඩ නැගීම පිහිනීම සහ සිනහවෙන් විනෝදයෙන් පිරුණු දවසක් සඳහා වෙරළට රැස් වූහ\tOn a sunny summer day a group of friends gathered at the beach for a fun-filled day of sandcastle building swimming and laughter\n","විචිත්‍රවත් මුහුදු වෙරළේ ගම්මානයකට අද්භූත ආගන්තුකයෙකු පැමිණියේ ඔවුන් සමඟ උද්දීපනයක් සහ නගර වැසියන්ගේ ජීවිතයට අනපේක්ෂිත පෙරළියක් ගෙන එයි\tIn a quaint seaside village a mysterious stranger arrived bringing with them an air of excitement and an unexpected twist to the townspeople's lives\n","අභිලාෂකාමී ලේඛකයෙකු වන තෝමස් සෑම අස්සක් මුල්ලක් නෑරම කීමට බලා සිටින කතන්දර ඇති උද්යෝගිමත් නගරයක කාර්යබහුල වීදිවල ආශ්වාදයක් ලබා ගත්තේය\tThomas an aspiring writer found inspiration in the bustling streets of a vibrant city where every corner held a story waiting to be told\n","මිථ්‍යා ජීවීන් සිටින රටක එම්බර් නම් තරුණ මකරෙක් මිත්‍රත්වයේ සහ ධෛර්‍යයේ වටිනා පාඩම් ඉගෙන ගනිමින් ඇගේ ගිනි හුස්ම පාලනය කිරීමට මහත් පරිශ්‍රමයක් දැරීය\tIn a land of mythical creatures a young dragon named Ember struggled to control her fiery breath while learning valuable lessons of friendship and courage\n"]}]},{"cell_type":"code","source":["import random\n","import string\n","import re\n","\n","text_pairs = []\n","\n","\n","for line in lines:\n","    if '\\t' in line:\n","        english, sinhala = line.split(\"\\t\")\n","        sinhala = \"[start]\" + sinhala.strip() + \"[end]\"\n","        text_pairs.append((english.strip(), sinhala))\n","for i in range(3):\n","    print(random.choice(text_pairs))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A3K4lA_TAiKM","outputId":"9866e6d7-dba8-420f-adaf-7c35206c31cb","executionInfo":{"status":"ok","timestamp":1713695047674,"user_tz":-330,"elapsed":551,"user":{"displayName":"Harindu Chiranjaya","userId":"07786780909468288837"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["('සමහරවිට එයාට මතක නැති\\u200d හේතුව එයා කරපු දේ ඇත්ත නිසා වෙන්න ඇති', \"[start]maybe the reason he doesn't remember is because what he did was true[end]\")\n","('නෑ මට සමාවෙන්න', \"[start]not i'm very sorry[end]\")\n","('වැඩි වෙලාවක් ඉගෙන ගන්න යෙදෙව්වානම්', '[start]if you spent more time studying[end]')\n"]}]},{"cell_type":"code","source":["import random\n","random.shuffle(text_pairs)"],"metadata":{"id":"OiFINLcyH6SQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_val_samples =int(0.15*len(text_pairs))\n","num_train_samples=len(text_pairs) -2* num_val_samples\n","train_pairs = text_pairs[:num_train_samples]\n","val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n","test_pairs = text_pairs[num_train_samples + num_val_samples:]"],"metadata":{"id":"ehIcgOPECGU0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Total sentences:\",len(text_pairs))\n","print(\"Training set size:\",len(train_pairs))\n","print(\"Validation set size:\",len(val_pairs))\n","print(\"Testing set size:\",len(test_pairs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eSXWkCSiIbFd","outputId":"b9c94910-1a56-4382-bd6b-ea4bc0183b08","executionInfo":{"status":"ok","timestamp":1713695054358,"user_tz":-330,"elapsed":4,"user":{"displayName":"Harindu Chiranjaya","userId":"07786780909468288837"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total sentences: 80633\n","Training set size: 56445\n","Validation set size: 12094\n","Testing set size: 12094\n"]}]},{"cell_type":"code","source":["len(train_pairs)+len(val_pairs)+len(test_pairs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_InNACfgIypi","outputId":"450f43cf-ab8a-4afd-e880-d788e9581cdf","executionInfo":{"status":"ok","timestamp":1713695056888,"user_tz":-330,"elapsed":3,"user":{"displayName":"Harindu Chiranjaya","userId":"07786780909468288837"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["80633"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["strip_chars = string.punctuation + \"¿\"\n","strip_chars = strip_chars.replace(\"[\", \"\")\n","strip_chars = strip_chars.replace(\"]\", \"\")"],"metadata":{"id":"i3SM4fSJI1j8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["f\"[{re.escape(strip_chars)}]\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"5oJtiTLnI4FD","outputId":"f23be54d-38d4-449a-e8dc-ee24fba043ef","executionInfo":{"status":"ok","timestamp":1713695061646,"user_tz":-330,"elapsed":6,"user":{"displayName":"Harindu Chiranjaya","userId":"07786780909468288837"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'[!\"\\\\#\\\\$%\\\\&\\'\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\./:;<=>\\\\?@\\\\\\\\\\\\^_`\\\\{\\\\|\\\\}\\\\~¿]'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["from tensorflow import keras\n","from tensorflow.keras import layers\n","def custom_standardization(input_string):\n","  lowercase = tf.strings.lower(input_string)\n","  return tf.strings.regex_replace(\n","    lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n","vocab_size = 15000\n","sequence_length = 20\n","source_vectorization = layers.TextVectorization(\n","  max_tokens=vocab_size,\n","  output_mode=\"int\",\n","  output_sequence_length=sequence_length,\n",")\n","target_vectorization = layers.TextVectorization(\n","  max_tokens=vocab_size,\n","  output_mode=\"int\",\n","  output_sequence_length=sequence_length + 1,\n","  standardize=custom_standardization,\n",")\n","train_english_texts = [pair[0] for pair in train_pairs]\n","train_spanish_texts = [pair[1] for pair in train_pairs]"],"metadata":{"id":"e082vYLkI8WY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["source_vectorization.adapt(train_english_texts)\n","target_vectorization.adapt(train_spanish_texts)"],"metadata":{"id":"h5kQGr9iJebo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size=64\n","\n","def format_dataset(eng,spa):\n","  eng=source_vectorization(eng)\n","  spa=target_vectorization(spa)\n","  return({\n","      \"english\":eng,\n","      \"sinhala\":spa[:,:-1],\n","  },spa[:,1:])\n","\n","def make_dataset(pairs):\n","  eng_texts, spa_texts =zip(*pairs)\n","  eng_texts =list(eng_texts)\n","  spa_texts =list(spa_texts)\n","  dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n","  dataset = dataset.batch(batch_size)\n","  dataset = dataset.map(format_dataset, num_parallel_calls=4)\n","  return dataset.shuffle(2048).prefetch(16).cache()\n","\n","train_ds = make_dataset(train_pairs)\n","val_ds = make_dataset(val_pairs)\n","\n","for inputs, targets in train_ds.take(1):\n","  print(f\"inputs['english'].shape:{inputs['english'].shape}\")\n","  print(f\"inputs['sinhala'].shape:{inputs['sinhala'].shape}\")\n","  print(f\"targets.shape:{targets.shape}\")\n","\n","  inputs['english'].shape:  (64, 20)\n","  inputs['sinhala'].shape: (64, 20)\n","  targets.shape: (64, 20)\n","print(list(train_ds.as_numpy_iterator())[50])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u1GdUR8fJmYn","outputId":"ffe5970d-0f42-438a-f58e-72b0f9402752","executionInfo":{"status":"ok","timestamp":1713695086813,"user_tz":-330,"elapsed":3785,"user":{"displayName":"Harindu Chiranjaya","userId":"07786780909468288837"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs['english'].shape:(64, 20)\n","inputs['sinhala'].shape:(64, 20)\n","targets.shape:(64, 20)\n","({'english': array([[   3,  270,  932, ...,    0,    0,    0],\n","       [ 182,   48,  219, ...,    0,    0,    0],\n","       [ 691, 1775,    0, ...,    0,    0,    0],\n","       ...,\n","       [  59, 2934,    0, ...,    0,    0,    0],\n","       [   3,    7,   78, ...,    0,    0,    0],\n","       [   9,   34,   23, ...,    0,    0,    0]]), 'sinhala': array([[ 300,    2,    9, ...,    0,    0,    0],\n","       [  98,  261,   23, ...,    0,    0,    0],\n","       [7023,    1,    0, ...,    0,    0,    0],\n","       ...,\n","       [ 584,   13, 1689, ...,    0,    0,    0],\n","       [  14,   55,   13, ...,    0,    0,    0],\n","       [  58,   27,  121, ...,    0,    0,    0]])}, array([[   2,    9,   31, ...,    0,    0,    0],\n","       [ 261,   23,  404, ...,    0,    0,    0],\n","       [   1,    0,    0, ...,    0,    0,    0],\n","       ...,\n","       [  13, 1689,    0, ...,    0,    0,    0],\n","       [  55,   13, 2417, ...,    0,    0,    0],\n","       [  27,  121,  336, ...,    0,    0,    0]]))\n"]}]},{"cell_type":"code","source":["\n","import string\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers"],"metadata":{"id":"ZRe3GQ6tNFWQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import layers\n","\n","class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim)\n","        self.dense_proj = tf.keras.Sequential([\n","            layers.Dense(dense_dim, activation=\"relu\"),\n","            layers.Dense(embed_dim),\n","        ])\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","\n","    def call(self, inputs, mask=None):\n","        if mask is not None:\n","            mask = mask[:, tf.newaxis, :]\n","        attention_output = self.attention(\n","            inputs, inputs, attention_mask=mask)\n","        proj_input = self.layernorm_1(inputs + attention_output)\n","        proj_output = self.dense_proj(proj_input)\n","        return self.layernorm_2(proj_input + proj_output)\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"num_heads\": self.num_heads,\n","            \"dense_dim\": self.dense_dim,\n","        })\n","        return config\n"],"metadata":{"id":"p3-O1MbFMCiz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import layers\n","\n","class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.attention_2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.dense_proj = tf.keras.Sequential([\n","            layers.Dense(dense_dim, activation=\"relu\"),\n","            layers.Dense(embed_dim),\n","        ])\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.layernorm_3 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"num_heads\": self.num_heads,\n","            \"dense_dim\": self.dense_dim,\n","        })\n","        return config\n","\n","    def get_causal_attention_mask(self, inputs):\n","        input_shape = tf.shape(inputs)\n","        batch_size, sequence_length = input_shape[0], input_shape[1]\n","        i = tf.range(sequence_length)[:, tf.newaxis]\n","        j = tf.range(sequence_length)\n","        mask = tf.cast(i >= j, dtype=\"int32\")\n","        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n","        mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], axis=0)\n","        return tf.tile(mask, mult)\n","\n","    def call(self, inputs, encoder_outputs, mask=None):\n","        causal_mask = self.get_causal_attention_mask(inputs)\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n","            padding_mask = tf.minimum(padding_mask, causal_mask)\n","        else:\n","            padding_mask = mask\n","        attention_output_1 = self.attention_1(\n","            query=inputs,\n","            value=inputs,\n","            key=inputs,\n","            attention_mask=causal_mask\n","        )\n","        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n","        attention_output_2 = self.attention_2(\n","            query=attention_output_1,\n","            value=encoder_outputs,\n","            key=encoder_outputs,\n","            attention_mask=padding_mask,\n","        )\n","        attention_output_2 = self.layernorm_2(attention_output_1 + attention_output_2)\n","        proj_output = self.dense_proj(attention_output_2)\n","        return self.layernorm_3(attention_output_2 + proj_output)\n"],"metadata":{"id":"nGguNiRsN6RY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import layers\n","\n","class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n","        super().__init__(**kwargs)\n","        self.token_embeddings = layers.Embedding(input_dim=input_dim, output_dim=output_dim)\n","        self.position_embeddings = layers.Embedding(input_dim=sequence_length, output_dim=output_dim)\n","        self.sequence_length = sequence_length\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","\n","    def call(self, inputs):\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=length, delta=1)\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.position_embeddings(positions)\n","        return embedded_tokens + embedded_positions\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return tf.math.not_equal(inputs, 0)\n","\n","    def get_config(self):\n","        config = super(PositionalEmbedding, self).get_config()\n","        config.update({\n","            \"output_dim\": self.output_dim,\n","            \"sequence_length\": self.sequence_length,\n","            \"input_dim\": self.input_dim,\n","        })\n","        return config\n"],"metadata":{"id":"KQzl_V5kOUi6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embed_dim = 256\n","dense_dim = 2048\n","num_heads = 8\n","\n","\n","# Define the encoder inputs\n","encoder_inputs = tf.keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n","\n","# Add positional embedding to the encoder inputs\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n","\n","# Encode the inputs using TransformerEncoder\n","encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n","\n","# Define the decoder inputs\n","decoder_inputs = tf.keras.Input(shape=(None,), dtype=\"int64\", name=\"sinhala\")\n","\n","# Add positional embedding to the decoder inputs\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n","\n","# Decode the inputs using TransformerDecoder\n","x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n","\n","# Apply dropout\n","x = layers.Dropout(0.5)(x)\n","\n","# Generate decoder outputs\n","decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n","\n","# Create the transformer model\n","transformer = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","transformer.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"llCLVX_OOkET","outputId":"b87b47b9-7252-433e-84db-6bed48d0001c","executionInfo":{"status":"ok","timestamp":1713695113181,"user_tz":-330,"elapsed":1887,"user":{"displayName":"Harindu Chiranjaya","userId":"07786780909468288837"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," english (InputLayer)        [(None, None)]               0         []                            \n","                                                                                                  \n"," sinhala (InputLayer)        [(None, None)]               0         []                            \n","                                                                                                  \n"," positional_embedding (Posi  (None, None, 256)            3845120   ['english[0][0]']             \n"," tionalEmbedding)                                                                                 \n","                                                                                                  \n"," positional_embedding_1 (Po  (None, None, 256)            3845120   ['sinhala[0][0]']             \n"," sitionalEmbedding)                                                                               \n","                                                                                                  \n"," transformer_encoder (Trans  (None, None, 256)            3155456   ['positional_embedding[0][0]']\n"," formerEncoder)                                                                                   \n","                                                                                                  \n"," transformer_decoder (Trans  (None, None, 256)            5259520   ['positional_embedding_1[0][0]\n"," formerDecoder)                                                     ',                            \n","                                                                     'transformer_encoder[0][0]'] \n","                                                                                                  \n"," dropout (Dropout)           (None, None, 256)            0         ['transformer_decoder[0][0]'] \n","                                                                                                  \n"," dense_4 (Dense)             (None, None, 15000)          3855000   ['dropout[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 19960216 (76.14 MB)\n","Trainable params: 19960216 (76.14 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["transformer.compile(optimizer=\"rmsprop\",\n","                    loss=\"sparse_categorical_crossentropy\",\n","                    metrics=[\"accuracy\"])\n","\n","transformer.fit(train_ds, epochs=30, validation_data=val_ds)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0gHl79wMPeZ8","outputId":"c7ebbbbe-cb04-43b8-cb37-01c8236ce593","executionInfo":{"status":"ok","timestamp":1713697182860,"user_tz":-330,"elapsed":2063174,"user":{"displayName":"Harindu Chiranjaya","userId":"07786780909468288837"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","882/882 [==============================] - 78s 78ms/step - loss: 5.0536 - accuracy: 0.2915 - val_loss: 4.1291 - val_accuracy: 0.3758\n","Epoch 2/30\n","882/882 [==============================] - 60s 69ms/step - loss: 4.1761 - accuracy: 0.3846 - val_loss: 3.7370 - val_accuracy: 0.4247\n","Epoch 3/30\n","882/882 [==============================] - 60s 69ms/step - loss: 3.8039 - accuracy: 0.4323 - val_loss: 3.5670 - val_accuracy: 0.4527\n","Epoch 4/30\n","882/882 [==============================] - 65s 73ms/step - loss: 3.5390 - accuracy: 0.4684 - val_loss: 3.4973 - val_accuracy: 0.4647\n","Epoch 5/30\n","882/882 [==============================] - 61s 69ms/step - loss: 3.3383 - accuracy: 0.4955 - val_loss: 3.4583 - val_accuracy: 0.4736\n","Epoch 6/30\n","882/882 [==============================] - 61s 69ms/step - loss: 3.1808 - accuracy: 0.5193 - val_loss: 3.4867 - val_accuracy: 0.4769\n","Epoch 7/30\n","882/882 [==============================] - 61s 69ms/step - loss: 3.0576 - accuracy: 0.5391 - val_loss: 3.4654 - val_accuracy: 0.4834\n","Epoch 8/30\n","882/882 [==============================] - 64s 73ms/step - loss: 2.9580 - accuracy: 0.5565 - val_loss: 3.4768 - val_accuracy: 0.4896\n","Epoch 9/30\n","882/882 [==============================] - 65s 73ms/step - loss: 2.8714 - accuracy: 0.5705 - val_loss: 3.5259 - val_accuracy: 0.4908\n","Epoch 10/30\n","882/882 [==============================] - 60s 68ms/step - loss: 2.7959 - accuracy: 0.5847 - val_loss: 3.5294 - val_accuracy: 0.4927\n","Epoch 11/30\n","882/882 [==============================] - 60s 69ms/step - loss: 2.7299 - accuracy: 0.5965 - val_loss: 3.5398 - val_accuracy: 0.5002\n","Epoch 12/30\n","882/882 [==============================] - 60s 69ms/step - loss: 2.6720 - accuracy: 0.6072 - val_loss: 3.5905 - val_accuracy: 0.4964\n","Epoch 13/30\n","882/882 [==============================] - 60s 68ms/step - loss: 2.6199 - accuracy: 0.6172 - val_loss: 3.6116 - val_accuracy: 0.4988\n","Epoch 14/30\n","882/882 [==============================] - 60s 69ms/step - loss: 2.5779 - accuracy: 0.6258 - val_loss: 3.6485 - val_accuracy: 0.4959\n","Epoch 15/30\n","882/882 [==============================] - 61s 69ms/step - loss: 2.5357 - accuracy: 0.6331 - val_loss: 3.6501 - val_accuracy: 0.5018\n","Epoch 16/30\n","882/882 [==============================] - 60s 68ms/step - loss: 2.5003 - accuracy: 0.6394 - val_loss: 3.6307 - val_accuracy: 0.4970\n","Epoch 17/30\n","882/882 [==============================] - 61s 69ms/step - loss: 2.4619 - accuracy: 0.6467 - val_loss: 3.6628 - val_accuracy: 0.5013\n","Epoch 18/30\n","882/882 [==============================] - 61s 69ms/step - loss: 2.4290 - accuracy: 0.6534 - val_loss: 3.6930 - val_accuracy: 0.5026\n","Epoch 19/30\n","882/882 [==============================] - 60s 69ms/step - loss: 2.3991 - accuracy: 0.6584 - val_loss: 3.6968 - val_accuracy: 0.5026\n","Epoch 20/30\n","882/882 [==============================] - 61s 69ms/step - loss: 2.3720 - accuracy: 0.6634 - val_loss: 3.7243 - val_accuracy: 0.5023\n","Epoch 21/30\n","882/882 [==============================] - 64s 73ms/step - loss: 2.3361 - accuracy: 0.6704 - val_loss: 3.7382 - val_accuracy: 0.5012\n","Epoch 22/30\n","882/882 [==============================] - 64s 73ms/step - loss: 2.3146 - accuracy: 0.6743 - val_loss: 3.7975 - val_accuracy: 0.5063\n","Epoch 23/30\n","882/882 [==============================] - 61s 69ms/step - loss: 2.2881 - accuracy: 0.6785 - val_loss: 3.7763 - val_accuracy: 0.5036\n","Epoch 24/30\n","882/882 [==============================] - 60s 69ms/step - loss: 2.2585 - accuracy: 0.6856 - val_loss: 3.7844 - val_accuracy: 0.5069\n","Epoch 25/30\n","882/882 [==============================] - 60s 68ms/step - loss: 2.2337 - accuracy: 0.6888 - val_loss: 3.8421 - val_accuracy: 0.5054\n","Epoch 26/30\n","882/882 [==============================] - 64s 73ms/step - loss: 2.2104 - accuracy: 0.6925 - val_loss: 3.8918 - val_accuracy: 0.4978\n","Epoch 27/30\n","882/882 [==============================] - 61s 69ms/step - loss: 2.1886 - accuracy: 0.6959 - val_loss: 3.8806 - val_accuracy: 0.5042\n","Epoch 28/30\n","882/882 [==============================] - 61s 69ms/step - loss: 2.1704 - accuracy: 0.7000 - val_loss: 3.9197 - val_accuracy: 0.5048\n","Epoch 29/30\n","882/882 [==============================] - 65s 73ms/step - loss: 2.1502 - accuracy: 0.7033 - val_loss: 3.9168 - val_accuracy: 0.5000\n","Epoch 30/30\n","882/882 [==============================] - 61s 69ms/step - loss: 2.1374 - accuracy: 0.7059 - val_loss: 3.9234 - val_accuracy: 0.5059\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7c7d1210e4a0>"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["import numpy as np\n","\n","# Define the vocabulary and index lookup for Spanish\n","spa_vocab = target_vectorization.get_vocabulary()\n","spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n","max_decoded_sentence_length = 20\n","\n","def decode_sequence(input_sentence):\n","    tokenized_input_sentence = source_vectorization([input_sentence])\n","    decoded_sentence = \"[start]\"\n","\n","    for i in range(max_decoded_sentence_length):\n","        tokenized_target_sentence = target_vectorization([decoded_sentence])[:, :-1]\n","        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n","        sampled_token_index = np.argmax(predictions[0, i, :])\n","        sampled_token = spa_index_lookup[sampled_token_index]\n","        decoded_sentence += \" \" + sampled_token\n","\n","        if sampled_token == \"[end]\":\n","            break\n","\n","    return decoded_sentence\n","\n","test_eng_texts = [pair[0] for pair in test_pairs]\n","\n","for _ in range(20):\n","    input_sentence = random.choice(test_eng_texts)\n","    print(\"-\")\n","    print(input_sentence)\n","    print(decode_sequence(input_sentence))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oobEWTErhTCp","outputId":"10eafe36-b7f5-48d4-da6c-8a72f651ef71","executionInfo":{"status":"ok","timestamp":1713697227681,"user_tz":-330,"elapsed":24347,"user":{"displayName":"Harindu Chiranjaya","userId":"07786780909468288837"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-\n","එතකොට ඩැඩී ගෙදර එනවාද\n","[start] on [UNK] is coming[end]                \n","-\n","ඇය මැඩපවත්වගෙන ඉන්නේ වෙන කෙනෙක් එක්ක සම්බන්ධ නිසා\n","[start] from im with another person[end]               \n","-\n","මට ඕනේ අපේ NSG කමාන්ඩෝස්ලා ජනාධිපතිතුමාවම ෆෝකස් කරන් ඉන්නවට\n","[start] from [UNK] our [UNK] will [UNK] will be taken away[end]          \n","-\n","ඒ කිව්ව්\n","[start] from that[end]                  \n","-\n","මේ පාර ඔයාලා දෙන්නාම\n","[start] two of you both of you[end]              \n","-\n","මම පොරොන්දු වෙනවා මේකෙන් පස්සේ\n","[start] i promise[end]                  \n","-\n","සජීවීව අපේ චැනල් එකේ\n","[start] on our channel[end]                 \n","-\n","මගේ පුතා\n","[start] son[end]                   \n","-\n","කොහොමද ඔයාට\n","[start] how you[end]                  \n","-\n","ඉතිං ඒ ඔයාලා නේද     නෑ\n","[start] thats isnt it[end]                 \n","-\n","ඝර්ෂණ නිරෝධය ඒක තමයි විදිය\n","[start] way it was the way to [UNK]             \n","-\n","හායි ඇලිසන්\n","[start] [UNK]                   \n","-\n","කම්මුල දෙන්න\n","[start] from [UNK]                  \n","-\n","මම දෙන්නම් තමුසෙට දර්ශනයක්\n","[start] will give me [UNK]                \n","-\n","අයියෝ  නියමයි\n","[start] on great[end]                  \n","-\n","මේ වගේ පිස්සු ලෝකයක සිහිබුද්ධියෙන් වැඩ කරන්න පුලුවන් පිස්සෙක්ට විතරයි\n","[start] from kind of work can [UNK] is not crazy in this country[end]        \n","-\n","හරි අන්න හරි\n","[start] right right[end]                  \n","-\n","ඇට     ශුක්‍රාණු දෙයියනේ\n","[start] [UNK] my joke[end]                 \n","-\n","ඒත් ගිවිසුම ගිවිසුමමයි\n","[start] problem at it[end]                 \n","-\n","දයාබර ඒමී දන්නවාද\n","[start] from front of [UNK]                \n"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","\n","sin_vocab = target_vectorization.get_vocabulary()\n","sin_index_lookup = dict(zip(range(len(sin_vocab)), sin_vocab))\n","max_decoded_sentence_length = 20\n","\n","def decode_sequence(input_sentence):\n","    tokenized_input_sentence = source_vectorization([input_sentence])\n","    decoded_sentence = \"[start]\"\n","\n","    for i in range(max_decoded_sentence_length):\n","        tokenized_target_sentence = target_vectorization([decoded_sentence])[:, :-1]\n","        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n","        sampled_token_index = np.argmax(predictions[0, i, :])\n","        sampled_token = sin_index_lookup[sampled_token_index]\n","        decoded_sentence += \" \" + sampled_token\n","\n","        if sampled_token == \"[end]\":\n","            break\n","\n","    return decoded_sentence\n","\n","test_eng_texts = [pair[0] for pair in test_pairs]\n","\n","for _ in range(20):\n","    input_sentence = random.choice(test_eng_texts)\n","    print(\"-\")\n","    print(\"English: \", input_sentence)\n","    print(\"Sinhala: \", decode_sequence(input_sentence))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ffe0dzs6hldz","outputId":"260736ed-ec87-4316-fa2e-7df434e980f7","executionInfo":{"status":"ok","timestamp":1713697263535,"user_tz":-330,"elapsed":24804,"user":{"displayName":"Harindu Chiranjaya","userId":"07786780909468288837"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-\n","English:  එහෙනම් කන්න එපා\n","Sinhala:  [start] eat no[end]                  \n","-\n","English:  බීබොට්\n","Sinhala:  [start]                    \n","-\n","English:  හොඳයි මෑම්\n","Sinhala:  [start] maam[end]                   \n","-\n","English:  පරිවර්තනය සහ උපසිරැසි\n","Sinhala:  [start] from [UNK]                  \n","-\n","English:  කවුද අහන්නේ\n","Sinhala:  [start] ask[end]                   \n","-\n","English:  අන්ශ්යමු\n","Sinhala:  [start] from [UNK]                  \n","-\n","English:  ඒකෙන් හොයන්න තියෙන ඉඩ අඩු කරනවා නේද\n","Sinhala:  [start] from matter what you [UNK]               \n","-\n","English:  මට අවකලණ සමීකරණ පන්තිය තියෙනවා තව විනාඩි 15කින්\n","Sinhala:  [start] from [UNK] minutes in class will meet another same now[end]          \n","-\n","English:  ඔයාට ඔයාගේ පක්ෂපාතිත්වය වෙනුවෙන් තෑග්ගක් හම්බවේවි\n","Sinhala:  [start] from your gift for your gift for it[end]            \n","-\n","English:  අවසන් දිනය\n","Sinhala:  [start] at [UNK]                  \n","-\n","English:  මොනාද අනේ\n","Sinhala:  [start] please[end]                   \n","-\n","English:  හෙස්පෙරා මේ ගහ දිහා බලන්න\n","Sinhala:  [start] at this tree[end]                 \n","-\n","English:  මේ නේමර්\n","Sinhala:  [start] from this [UNK]                 \n","-\n","English:  තව මගෙන්\n","Sinhala:  [start] from me[end]                  \n","-\n","English:  ඔයාලා තරම් නෑ  හරි අපි වැඩේ පටන් ගමු\n","Sinhala:  [start] right way we guys lets do the job[end]            \n","-\n","English:  මොකක්    ඇත්තට    ඔයාට පිස්සු\n","Sinhala:  [start] crazy[end]                   \n","-\n","English:  කුඩා ප්‍රමාණයේ කැබලි කිහිපයක් නිව්යෝර්ක් ඉහල ප්‍රදේශයට\n","Sinhala:  [start] few small [UNK] and new drink to can wont care[end]          \n","-\n","English:  මොකද\n","Sinhala:  [start] up[end]                   \n","-\n","English:  ඒ වාහනය අරගෙන ගිය කෙනාත් මම දැකලා තියෙනවා\n","Sinhala:  [start] [UNK] have seen last i saw it[end]             \n","-\n","English:  ඒත් ඔයා එයාලාව පැරැද්දුවා හැමවෙලාවෙම\n","Sinhala:  [start] in [UNK] them[end]                 \n"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","\n","sin_vocab = target_vectorization.get_vocabulary()\n","sin_index_lookup = dict(zip(range(len(sin_vocab)), sin_vocab))\n","max_decoded_sentence_length = 20\n","\n","def decode_sequence(input_sentence):\n","    tokenized_input_sentence = source_vectorization([input_sentence])\n","    decoded_sentence = \"[start]\"\n","\n","    for i in range(max_decoded_sentence_length):\n","        tokenized_target_sentence = target_vectorization([decoded_sentence])[:, :-1]\n","        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n","        sampled_token_index = np.argmax(predictions[0, i, :])\n","        sampled_token = sin_index_lookup[sampled_token_index]\n","        decoded_sentence += \" \" + sampled_token\n","\n","        if sampled_token == \"[end]\":\n","            break\n","\n","    return decoded_sentence\n","\n","# Get English input from the user\n","input_sentence = input(\"Enter an English sentence: \")\n","\n","# Translate the input sentence to Sinhala\n","translated_sentence = decode_sequence(input_sentence)\n","\n","# Print the translated sentence\n","print(\"Sinhala translation:\", translated_sentence)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Br7yrudhyLK","outputId":"67aa22a6-6f9c-4a5a-d331-3d17b8ace17d","executionInfo":{"status":"ok","timestamp":1713697276072,"user_tz":-330,"elapsed":6585,"user":{"displayName":"Harindu Chiranjaya","userId":"07786780909468288837"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter an English sentence: කොහොමද ඔයාට\n","Sinhala translation: [start] how you[end]                  \n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"hyapq-LsiZaB"},"execution_count":null,"outputs":[]}]}